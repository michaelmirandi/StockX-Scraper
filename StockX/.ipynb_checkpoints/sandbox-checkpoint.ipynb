{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import StockX\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from pymongo import MongoClient\n",
    "import datetime\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_proxies = [\"http://6f14b70bb8184faba7b4d76486dc78c5:@proxy.crawlera.com:8010/\"]\n",
    "\n",
    "lst_headers = ['Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:36.0) Gecko/20100101 Firefox/36.0',\n",
    "              'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:37.0) Gecko/20100101 Firefox/37.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SoleML_V2.StockX_Scraper(lst_proxies, lst_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s.reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.product_scraper('https://stockx.com/nike/foamposite/top-selling?size_types=men', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_information(product_link, header):\n",
    "    '''\n",
    "    Function: get_product_information()\n",
    "    Parameters: product_link (link of product to gather)\n",
    "    Returns: Nothing\n",
    "    Purpose: To collect the product information for the product_scraper function\n",
    "    '''\n",
    "\n",
    "    r = requests.get(product_link, headers=header)\n",
    "    # parse the request into a BS object using BS4\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    style_num = soup.find(\"span\", {\"data-testid\":'product-detail-style'}).text\n",
    "    script_tag = json.loads(soup.find_all('script', type='application/ld+json')[3].text)\n",
    "    dict_product = {'_id': script_tag['sku'], 'name': script_tag['name'], \n",
    "                    'model': script_tag['model'], 'style': style_num, color: script_tag['color'],\n",
    "                    'releasedate': datetime.datetime.strptime(script_tag['releaseDate'], '%Y-%m-%d')}\n",
    "    print(dict_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\"User-Agent\": 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:37.0) Gecko/20100101 Firefox/37.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_product_information('https://stockx.com/nike-air-air-foamposite-pro-sequoia', header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('https://stockx.imgix.net/Nike-Air-Foamposite-Pro-Sequoia.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myquery = { \"brand\": \"Nike\" }\n",
    "\n",
    "x = col_products.delete_many(myquery)\n",
    "\n",
    "print(x.deleted_count, \" documents deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class req:\n",
    "    def __init__(self, h, p):\n",
    "        self.proxy = {'http': \"http://\" + p,}\n",
    "        self.header = {\"User-Agent\": h}\n",
    "        self.valid = False\n",
    "\n",
    "    def validate(self, test_url):\n",
    "        try:\n",
    "            r = requests.get(test_url, headers=self.header, proxies=self.proxy)\n",
    "            self.valid = (r.status_code == 200)\n",
    "        except Exception:\n",
    "            self.valid = False\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.req object at 0x000002B862EADEC8>\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e9cfd2482838>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'hTJUNS'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Blocked'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "home_page = 'https://stockx.com/nike/lebron/1?size_types=men'\n",
    "test = req(p=\"http://b92489f48f554678b45f7c508baaf463:@proxy.crawlera.com:8010/\",\n",
    "          h='Mozilla/5.0 (Linux; U; Android 2.2) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1')\n",
    "proxy = test.proxy\n",
    "header = test.header\n",
    "print(test)\n",
    "r = requests.get(home_page + '&page=' + str(1), proxies=proxy, headers=header)\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "results = soup.find_all('a', {'class': 'hTJUNS'})\n",
    "if len(results) == 0:\n",
    "    return 1\n",
    "else:\n",
    "    return results[-1].text\n",
    "\n",
    "if r.status_code != 200: print('Blocked')\n",
    "r.status_code\n",
    "#BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_test=[x for x in range(5)]\n",
    "lst_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(lst_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "    home_page = 'https://stockx.com/nike/lebron/size_types=men'\n",
    "    test = req(p=\"http://b92489f48f554678b45f7c508baaf463:@proxy.crawlera.com:8010/\",\n",
    "              h='Mozilla/5.0 (Linux; U; Android 2.2) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1')\n",
    "    proxy = test.proxy\n",
    "    header = test.header\n",
    "    r = requests.get(home_page + '&page=' + str(1), proxies=proxy, headers=header)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    results = soup.find_all('a', {'class': 'hTJUNS'})\n",
    "    if len(results) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return int(results[-1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from itertools import cycle\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "class req:\n",
    "    def __init__(self, h):\n",
    "        self.header = {\"User-Agent\": h}\n",
    "\n",
    "    def validate(self, test_url):\n",
    "        try:\n",
    "            r = requests.get(test_url, headers=self.header, proxies=self.proxy)\n",
    "            return (r.status_code == 200)\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "\n",
    "class StockX_Scraper():\n",
    "    '''\n",
    "    Class based view for quickly creating scrapers that can be threaded.\n",
    "    '''\n",
    "    def __init__(self, proxy, lst_headers):\n",
    "        # can't stop me... try again https://www.perimeterx.com/whywasiblocked/\n",
    "        # set a list of headers to cycle through\n",
    "        print('Testing all proxy and headers for validity. This may take a while...')\n",
    "        self.reqs = []\n",
    "        for header in lst_headers:\n",
    "            r = req(header)\n",
    "            self.reqs.append(req(header))\n",
    "        self.req_pool = cycle(self.reqs)\n",
    "        self.lst_links = []\n",
    "        self.proxy = {'http': \"http://\" + proxy,}\n",
    "        self.client = MongoClient(\"mongodb+srv://soelml:WheresMyBelt29!@soleml-sandbox-s5fy7.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "        self.db = self.client.SoleML\n",
    "        self.col_products = self.db['products']\n",
    "        self.col_transactions = self.db['test_transactions']\n",
    "\n",
    "    def historical_scraper_main_002(self):\n",
    "        lst_failed = []\n",
    "        global total_scraped\n",
    "        total_scraped = 0\n",
    "        count = 0\n",
    "        for product in self.col_products.find():\n",
    "            count += 1\n",
    "            if count > 4:\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    self.get_historical_product_data_001(product['_id'])\n",
    "                except Exception as e:\n",
    "                    print('Error', str(e))\n",
    "                    print('Could not scrap:', product['name'])\n",
    "                    lst_failed.append(product['_id'])\n",
    "                    self.sleep(2, 3, 'Sleeping for')\n",
    "                    pass\n",
    "            print('Total scraped:', total_scraped)\n",
    "            print(lst_failed)\n",
    "\n",
    "    def get_historical_product_data_001(self, sku):\n",
    "        '''\n",
    "        Function: get_historical_product_data()\n",
    "        Parameters: sku (specific sku that you want to get information on)\n",
    "        Returns: Nothing\n",
    "        Purpose: To collect the historical data on a certain sku and store in MongoDB\n",
    "        Update for last time scraped...\n",
    "        '''\n",
    "\n",
    "        global total_scraped\n",
    "        df_historicaldata = pd.DataFrame()\n",
    "        page_count = 1\n",
    "        while True:\n",
    "\n",
    "            url = 'https://stockx.com/api/products/' + \\\n",
    "            sku + \\\n",
    "            '/activity?state=480&currency=USD&limit=20000&page='+ \\\n",
    "            str(page_count) + '&sort=createdAt&order=DESC&country=US'\n",
    "            count = 0\n",
    "            while True:\n",
    "                print('Getting valid request for ', url)\n",
    "                req = next(self.req_pool)\n",
    "                header = req.header\n",
    "                r = requests.get(url, headers=header, proxies=self.proxy)\n",
    "                print(r)\n",
    "                if r.status_code == 200: break\n",
    "                count += 1\n",
    "                if count > 3:\n",
    "                    count = 0\n",
    "                    self.sleep(90, 110, '---Resting for')\n",
    "\n",
    "            data_historical = r.json()\n",
    "            if 0 < data_historical['Pagination']['total'] < 20000:\n",
    "                df_historicaldata = df_historicaldata.append(pd.DataFrame(list(data_historical['ProductActivity'])))\n",
    "                page_count += 1\n",
    "                total_scraped += data_historical['Pagination']['total']\n",
    "                self.sleep(2, 3, 'Sleeping for')\n",
    "                break\n",
    "            elif (page_count - 1) * 20000 <= data_historical['Pagination']['total'] <= page_count * 20000:\n",
    "                    df_historicaldata = df_historicaldata.append(pd.DataFrame(list(data_historical['ProductActivity'])))\n",
    "                    page_count += 1\n",
    "                    self.sleep(2, 3, 'Sleeping for')\n",
    "            elif data_historical['Pagination']['total'] == 0:\n",
    "                break\n",
    "            else:\n",
    "                total_scraped += data_historical['Pagination']['total']\n",
    "                break\n",
    "\n",
    "        if df_historicaldata.empty: print('No records for', sku)\n",
    "        df_historicaldata['productId'] = df_historicaldata['productId'].apply(lambda x: sku)\n",
    "        df_historicaldata = df_historicaldata.rename(columns={'createdAt': 'soldAt'})\n",
    "        df_historicaldata = df_historicaldata[['productId', 'soldAt', 'shoeSize', 'amount']]\n",
    "        df_historicaldata = df_historicaldata.astype({'soldAt': 'datetime64[ns]'})\n",
    "        self.col_transactions.insert_many(df_historicaldata.to_dict(orient='records'))\n",
    "        self.sleep(2, 3, 'Sleeping for')\n",
    "\n",
    "    def product_link_scraper_001(self, home_page, start_page, end_page):\n",
    "        '''\n",
    "        Function: get_product_information_002_002()\n",
    "        Parameters: product_link (link of product to gather)\n",
    "        Returns: Nothing\n",
    "        Purpose: To collect the links for products in lst_links\n",
    "        Call this first to get the links of what you want to scrap\n",
    "        Need to fix to automatically detect the start and end pages...\n",
    "        '''\n",
    "        for page in range(start_page, end_page):\n",
    "            url = home_page + '&page=' + str(page)\n",
    "            count = 0\n",
    "            while True:\n",
    "                count += 1\n",
    "                print('Getting valid request for ', url)\n",
    "                req = next(self.req_pool)\n",
    "                header = req.header\n",
    "                r = requests.get(url, headers=header, proxies=self.proxy)\n",
    "                if r.status_code == 200: break\n",
    "                if count > 3:\n",
    "                    count = 0\n",
    "                    self.sleep(90, 110, '---Resting for')\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "            #print(soup)\n",
    "            results = soup.find_all('div', {'data-testid':'product-tile'})\n",
    "            for div_tag in results:\n",
    "                for a_tag in div_tag.find_all('a'):\n",
    "                    self.lst_links.append('https://stockx.com' + a_tag['href'])\n",
    "\n",
    "            self.sleep(2, 3, 'Sleeping for')\n",
    "\n",
    "\n",
    "    def product_info_main_001(self, home_page, start_page, end_page):\n",
    "        '''\n",
    "        Gather all product links, then get all product info from the links\n",
    "        '''\n",
    "        self.product_link_scraper_001(home_page, start_page, end_page)\n",
    "        random.shuffle(self.lst_links)\n",
    "\n",
    "        for link in self.lst_links:\n",
    "            try:\n",
    "                # inserts the product information into the products collection\n",
    "                self.get_product_information_002(link)\n",
    "                sleep_time = random.randint(2,4)\n",
    "                print('Sleeping for', sleep_time, 'seconds...')\n",
    "                time.sleep(sleep_time)\n",
    "            except Exception as e:\n",
    "                print('---Error:', e)\n",
    "                print('Could not get ', link)\n",
    "                self.sleep(2, 3, 'Sleeping for')\n",
    "                pass\n",
    "\n",
    "    def get_product_information_002(self, product_link):\n",
    "        '''\n",
    "        Function: get_product_information_002_002()\n",
    "        Parameters: product_link (link of product to gather)\n",
    "        Returns: Nothing\n",
    "        Purpose: To collect the product information for the product_scraper function\n",
    "        '''\n",
    "        count = 0\n",
    "        while True:\n",
    "            print('Getting valid request for ', product_link)\n",
    "            req = next(self.req_pool)\n",
    "            header = req.header\n",
    "            r = requests.get(product_link, headers=header, proxies=self.proxy)\n",
    "            if r.status_code == 200: break; time.sleep(1)\n",
    "            count += 1\n",
    "            if count > 2:\n",
    "                count = 0\n",
    "                self.sleep(90, 110, '--- Resting for')\n",
    "        # parse the request into a BS object using BS4\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        if soup.find(\"span\", {\"data-testid\":'product-detail-style'}) is not None:\n",
    "            style_num = soup.find(\"span\", {\"data-testid\":'product-detail-style'}).text\n",
    "        else:\n",
    "            style_num = None\n",
    "\n",
    "        script_tag = json.loads(soup.find_all('script', type='application/ld+json')[3].text)\n",
    "\n",
    "\n",
    "        if 'releaseDate' not in script_tag: script_tag['releaseDate'] = None\n",
    "        if '--' == script_tag['releaseDate']: script_tag['releaseDate'] = None\n",
    "        if script_tag['releaseDate'] is not None: script_tag['releaseDate'] = datetime.strptime(script_tag['releaseDate'], '%Y-%m-%d')\n",
    "        if 'sku' not in script_tag: script_tag['sku'] = None\n",
    "\n",
    "        dict_product = {'_id': script_tag['sku'], 'name': script_tag['name'], 'brand': script_tag['brand'],\n",
    "                        'model': script_tag['model'], 'style': style_num, 'color': script_tag['color'],\n",
    "                        'releasedate': script_tag['releaseDate']}\n",
    "        self.col_products.insert_one(dict_product)\n",
    "        print(\"Inserted\", script_tag['name'])\n",
    "\n",
    "    def sleep(self, lower_time, upper_time, message):\n",
    "        '''\n",
    "        Function to sleep for a set amount of time\n",
    "        '''\n",
    "        sleep_time = random.randint(90,110)\n",
    "        print(message, sleep_time, 'seconds...')\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    def main(self, home_page, start_page, end_page):\n",
    "        '''\n",
    "        Main function to call other functions that gather links, gather product info, and gather historical data\n",
    "        '''\n",
    "        self.product_info_main_001(home_page, start_page, end_page)\n",
    "        self.historical_scraper_main_002()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all proxy and headers for validity. This may take a while...\n"
     ]
    }
   ],
   "source": [
    "proxy = \"http://4659b951bb7b4c1c8366ae870c52c6d8:@proxy.crawlera.com:8010/\"\n",
    "lst_headers = ['Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:36.0) Gecko/20100101 Firefox/36.0',\n",
    "              'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:37.0) Gecko/20100101 Firefox/37.0']\n",
    "s = StockX_Scraper(proxy, lst_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.historical_scraper_main_002()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = MongoClient(\"mongodb+srv://soelml:WheresMyBelt29!@soleml-sandbox-s5fy7.mongodb.net/SoleML-Sandbox?retryWrites=true&w=majority\")\n",
    "db = client.SoleML\n",
    "col_transactions = db['transactions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for _ in col_transactions.find():\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myquery = { \"productId\": {\"$regex\": \".*\"} }\n",
    "\n",
    "col_transactions.delete_many(myquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
